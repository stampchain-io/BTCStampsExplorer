name: Newman Comprehensive Tests (Production Validation)

# This workflow validates production API endpoints on a schedule
# It does NOT run on code changes because Newman tests don't add value in CI without a dev server
# For local development: Use 'deno task check:api:comprehensive' to compare localhost vs production

on:
  # Remove push/PR triggers - Newman tests don't add value in CI without dev server
  # push:
  #   branches: [main, dev]
  #   paths:
  #     - 'tests/postman/collections/comprehensive.json'
  #     - 'tests/postman/collections/pagination-validation.json'
  #     - 'tests/postman/environments/comprehensive.json'
  #     - 'tests/postman/data/pagination-tests.json'
  #     - 'scripts/run-newman-comprehensive.sh'
  #     - 'scripts/run-newman-pagination-validation.sh'
  #     - 'scripts/analyze-newman-regression.js'
  #     - 'server/**'
  #     - 'routes/**'
  #     - 'lib/**'
  #     - '.github/workflows/newman-comprehensive-tests.yml'
  # pull_request:
  #   branches: [main, dev]
  #   paths:
  #     - 'tests/postman/collections/comprehensive.json'
  #     - 'tests/postman/collections/pagination-validation.json'
  #     - 'tests/postman/environments/comprehensive.json'
  #     - 'tests/postman/data/pagination-tests.json'
  #     - 'scripts/run-newman-comprehensive.sh'
  #     - 'scripts/run-newman-pagination-validation.sh'
  #     - 'scripts/analyze-newman-regression.js'
  #     - 'server/**'
  #     - 'routes/**'
  #     - 'lib/**'
  #     - '.github/workflows/newman-comprehensive-tests.yml'
  workflow_dispatch:
    inputs:
      test_folder:
        description: 'Specific test folder to run (e.g., "Stamps Endpoints")'
        required: false
        default: ''
      iterations:
        description: 'Number of iterations'
        required: false
        default: '1'
      verbose:
        description: 'Enable verbose output'
        required: false
        default: 'false'
  schedule:
    # Run daily at 2 AM UTC to validate production endpoints
    - cron: '0 2 * * *'

jobs:
  newman-comprehensive:
    name: Newman Integration Tests (Database Required)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    permissions:
      contents: read
      pull-requests: write
      issues: write
      
    env:
      CI: true
      NODE_ENV: test
      # Mock API keys for testing
      QUICKNODE_API_KEY: test-quicknode-key
      ANTHROPIC_API_KEY: test-anthropic-key
      PERPLEXITY_API_KEY: test-perplexity-key

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache npm dependencies
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-newman-${{ hashFiles('**/package.json') }}
          restore-keys: |
            ${{ runner.os }}-npm-newman-
            ${{ runner.os }}-npm-

      - name: Verify test files exist
        run: |
          echo "Verifying test collection and environment files..."
          if [ ! -f "tests/postman/collections/comprehensive.json" ]; then
            echo "âŒ Error: tests/postman/collections/comprehensive.json not found"
            echo "This file contains the comprehensive test collection (100+ tests)"
            exit 1
          fi
          
          if [ ! -f "tests/postman/environments/comprehensive.json" ]; then
            echo "âŒ Error: tests/postman/environments/comprehensive.json not found"
            echo "Creating default environment file..."
            mkdir -p tests/postman/environments
            echo '{
              "id": "comprehensive-env",
              "name": "Comprehensive Test Environment",
              "values": [
                {"key": "dev_base_url", "value": "http://localhost:8000", "enabled": true},
                {"key": "prod_base_url", "value": "https://stampchain.io", "enabled": true}
              ]
            }' > tests/postman/environments/comprehensive.json
          fi
          
          echo "âœ… Test files found"
          echo "Collection size: $(wc -l < tests/postman/collections/comprehensive.json) lines"

      - name: Setup Docker Compose
        run: |
          echo "Docker version:"
          docker --version
          echo "Docker Compose version:"
          docker compose version
          
          # Ensure reports directory exists
          mkdir -p reports/newman-comprehensive
          chmod -R 755 reports

      - name: Configure test parameters
        id: test-config
        run: |
          # Set test parameters based on workflow inputs or defaults
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "NEWMAN_FOLDER=${{ github.event.inputs.test_folder }}" >> $GITHUB_ENV
            echo "NEWMAN_ITERATIONS=${{ github.event.inputs.iterations }}" >> $GITHUB_ENV
            echo "NEWMAN_VERBOSE=${{ github.event.inputs.verbose }}" >> $GITHUB_ENV
          else
            echo "NEWMAN_ITERATIONS=1" >> $GITHUB_ENV
            echo "NEWMAN_VERBOSE=false" >> $GITHUB_ENV
          fi
          
          # Always set production URL
          echo "PROD_BASE_URL=https://stampchain.io" >> $GITHUB_ENV
          
          # CI Mode: Since we don't have a dev server running in CI,
          # we'll run production-only tests but still use the dual-endpoint collection
          # The collection will just hit production for both dev and prod URLs
          echo "DEV_BASE_URL=https://stampchain.io" >> $GITHUB_ENV
          echo "TEST_MODE=ci-production-only" >> $GITHUB_ENV
          
          # Note: For local development with dev server running, users should run:
          # DEV_BASE_URL=http://localhost:8000 PROD_BASE_URL=https://stampchain.io docker compose -f docker-compose.test.yml run --rm newman-comprehensive

      - name: Run Newman Comprehensive Tests
        id: newman-test
        run: |
          {
            echo "## Newman Integration Test Run"
            echo ""
            echo "âš ï¸ **Integration Test**: Tests live API endpoints with database access"
            echo ""
            echo "### Test Configuration"
            echo "- Collection: tests/postman/collections/comprehensive.json (100+ tests)"
            echo "- Coverage: 46/46 endpoints (100%)"
            echo "- Mode: ${{ env.TEST_MODE }}"
            echo "- DEV_BASE_URL: ${{ env.DEV_BASE_URL }}"
            echo "- PROD_BASE_URL: ${{ env.PROD_BASE_URL }}"
            echo "- Iterations: ${{ env.NEWMAN_ITERATIONS }}"
            
            if [ -n "${{ env.NEWMAN_FOLDER }}" ]; then
              echo "- Folder: ${{ env.NEWMAN_FOLDER }}"
            fi
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
          
          # Run the tests
          docker compose -f docker-compose.test.yml run --rm newman-comprehensive || TEST_EXIT_CODE=$?
          
          # Check for test results
          if [ -d "reports/newman-comprehensive" ]; then
            {
              echo "### Test Results"
              echo "Reports generated in: reports/newman-comprehensive/"
              ls -la reports/newman-comprehensive/
            } >> "$GITHUB_STEP_SUMMARY"
          fi
          
          exit ${TEST_EXIT_CODE:-0}
        continue-on-error: true

      - name: Fix Docker file permissions
        if: always()
        run: |
          # Docker containers run as root and change file ownership
          # This prevents EACCES errors in subsequent steps
          sudo chown -R $(id -u):$(id -g) . || true

      - name: Analyze Regression Results
        id: regression-analysis
        if: always()
        run: |
          echo "Running regression analysis..."

          # Run the analysis (uses only Node.js built-in modules, no npm install needed)
          node scripts/analyze-newman-regression.js || ANALYSIS_EXIT_CODE=$?

          # Check if analysis report was created
          if ls reports/newman-comprehensive/*-analysis.json 1>/dev/null 2>&1; then
            echo "âœ… Regression analysis completed"

            # Extract summary for GitHub
            LATEST_ANALYSIS=$(ls -t reports/newman-comprehensive/*-analysis.json | head -1)
            if [ -f "$LATEST_ANALYSIS" ]; then
              {
                echo ""
                echo "### Regression Analysis Summary"
                echo ""

                # Parse JSON and add to summary
                node -e "
                  const fs = require('fs');
                  const analysis = JSON.parse(fs.readFileSync('$LATEST_ANALYSIS', 'utf8'));
                  console.log('- Breaking Changes: ' + analysis.summary.breaking);
                  console.log('- Non-Breaking Changes: ' + analysis.summary.nonBreaking);
                  console.log('- Performance Issues: ' + analysis.summary.performanceIssues);
                  console.log('- Performance Improvements: ' + analysis.summary.performanceImprovements);
                "
              } >> "$GITHUB_STEP_SUMMARY"
            fi
          else
            echo "âš ï¸ No analysis reports found â€” Newman may not have generated output files"
          fi

          exit ${ANALYSIS_EXIT_CODE:-0}
        continue-on-error: true

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: newman-comprehensive-reports-${{ github.run_number }}
          path: reports/newman-comprehensive/
          retention-days: 30

      - name: Create PR comment with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            let comment = '## ðŸ§ª Newman Integration Test Results (API)\n\n';
            comment += 'âš ï¸ **Integration Test**: These tests hit live API endpoints and require database access.\n\n';
            
            // Test execution status
            const testStatus = '${{ steps.newman-test.outcome }}';
            const analysisStatus = '${{ steps.regression-analysis.outcome }}';
            
            if (testStatus === 'success') {
              comment += '### âœ… All API Tests Passed\n\n';
              comment += '- **Coverage**: 46/46 endpoints (100%)\n';
              comment += '- **Total Tests**: 92+\n';
            } else if (testStatus === 'failure') {
              comment += '### âŒ API Tests Failed\n\n';
              comment += 'Some tests did not pass. Please check the detailed report.\n\n';
            }
            
            // Regression analysis results
            if (analysisStatus === 'success' || analysisStatus === 'failure') {
              comment += '\n### ðŸ“Š Regression Analysis\n\n';
              
              try {
                // Find the latest analysis file
                const analysisFiles = fs.readdirSync('reports/newman-comprehensive/')
                  .filter(f => f.endsWith('-analysis.json'))
                  .sort((a, b) => b.localeCompare(a));
                
                if (analysisFiles.length > 0) {
                  const latestAnalysis = JSON.parse(
                    fs.readFileSync(`reports/newman-comprehensive/${analysisFiles[0]}`, 'utf8')
                  );
                  
                  const summary = latestAnalysis.summary;
                  
                  if (summary.breaking > 0) {
                    comment += `âš ï¸ **Breaking Changes Detected**: ${summary.breaking}\n\n`;
                    
                    // List breaking changes
                    if (latestAnalysis.regressions.breaking.length > 0) {
                      comment += '<details><summary>Breaking Changes Details</summary>\n\n';
                      latestAnalysis.regressions.breaking.slice(0, 5).forEach(bc => {
                        comment += `- **${bc.request}**: ${bc.assertion}\n`;
                        comment += `  - Error: ${bc.error}\n`;
                      });
                      if (latestAnalysis.regressions.breaking.length > 5) {
                        comment += `\n... and ${latestAnalysis.regressions.breaking.length - 5} more\n`;
                      }
                      comment += '\n</details>\n\n';
                    }
                  } else {
                    comment += 'âœ… **No Breaking Changes**\n\n';
                  }
                  
                  // Non-breaking changes
                  if (summary.nonBreaking > 0) {
                    comment += `â„¹ï¸ **Non-Breaking Changes**: ${summary.nonBreaking}\n`;
                    comment += 'These are typically new optional fields or formatting changes.\n\n';
                  }
                  
                  // Performance
                  if (summary.performanceIssues > 0) {
                    comment += `âš¡ **Performance Issues**: ${summary.performanceIssues}\n`;
                  }
                  if (summary.performanceImprovements > 0) {
                    comment += `ðŸš€ **Performance Improvements**: ${summary.performanceImprovements}\n`;
                  }
                }
              } catch (e) {
                comment += 'Could not parse regression analysis results.\n';
                console.error('Error parsing analysis:', e);
              }
            }
            
            // Add links
            comment += '\n### ðŸ“Ž Resources\n\n';
            comment += '- [Test Reports Artifact](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
            comment += '- [Newman Testing Documentation](docs/NEWMAN_COMPREHENSIVE_TESTING.md)\n';
            comment += '- [API Endpoint Coverage](docs/API_ENDPOINT_AUDIT.md)\n';
            
            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('Newman Integration Test Results')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Summarize results
        if: always()
        run: |
          echo "=== Newman Comprehensive Test Summary ==="

          # Report Newman test status
          if [ "${{ steps.newman-test.outcome }}" == "failure" ]; then
            echo "âš ï¸ Newman tests had assertion failures (check step summary for details)"
            echo "   Note: /api/internal/* endpoints return 403 by design in CI"
          else
            echo "âœ… Newman tests passed"
          fi

          # Report regression analysis status
          if [ "${{ steps.regression-analysis.outcome }}" == "failure" ]; then
            echo "âš ï¸ Regression analysis flagged issues (check artifacts for report)"
          else
            echo "âœ… Regression analysis completed"
          fi

          # This workflow is informational (scheduled monitoring only)
          # Results are in step summary and uploaded artifacts
          echo "âœ… Monitoring run complete"

  pagination-validation:
    name: Pagination & Data Validation Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || github.event_name == 'push'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Newman
        run: |
          npm install -g newman newman-reporter-html newman-reporter-json

      - name: Run Pagination Validation Tests
        id: pagination-test
        run: |
          echo "## Pagination Validation Test Run" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- Collection: tests/postman/collections/pagination-validation.json" >> $GITHUB_STEP_SUMMARY
          echo "- Test Data: postman-data-pagination-tests.json" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Run pagination validation tests
          ./scripts/run-newman-pagination-validation.sh || PAGINATION_EXIT_CODE=$?
          
          exit ${PAGINATION_EXIT_CODE:-0}
        continue-on-error: true

      - name: Upload pagination test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pagination-validation-reports-${{ github.run_number }}
          path: reports/newman-pagination-validation/
          retention-days: 30

  performance-benchmark:
    name: API Performance Benchmarking
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run performance benchmarks
        run: |
          echo "## Performance Benchmarking" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- Iterations: 3" >> $GITHUB_STEP_SUMMARY
          echo "- Delay between requests: 100ms" >> $GITHUB_STEP_SUMMARY
          echo "- Focus: Critical endpoints only" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Run performance-focused tests against production
          # (no dev server in CI, point both URLs at production)
          DEV_BASE_URL=https://stampchain.io \
          PROD_BASE_URL=https://stampchain.io \
          NEWMAN_ITERATIONS=3 \
          NEWMAN_DELAY_REQUEST=100 \
          NEWMAN_FOLDER="Stamps Endpoints" \
          docker compose -f docker-compose.test.yml run --rm newman-comprehensive

      - name: Store performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmark-${{ github.run_number }}
          path: reports/newman-comprehensive/
          retention-days: 90