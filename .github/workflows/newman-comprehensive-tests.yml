name: Newman Comprehensive Tests (Production Validation + Local Dev)

# This workflow includes:
# 1. Production validation (scheduled) - validates live stampchain.io endpoints
# 2. Local dev testing (push/PR) - runs Newman tests against local dev server with MySQL+Redis

on:
  push:
    branches: [main, dev]
    paths:
      - 'tests/postman/collections/comprehensive.json'
      - 'tests/postman/collections/pagination-validation.json'
      - 'tests/postman/environments/comprehensive.json'
      - 'tests/postman/data/pagination-tests.json'
      - 'scripts/run-newman-comprehensive.sh'
      - 'scripts/run-newman-pagination-validation.sh'
      - 'scripts/analyze-newman-regression.js'
      - 'scripts/test-schema.sql'
      - 'scripts/test-seed-data.sql'
      - 'server/**'
      - 'routes/**'
      - 'lib/**'
      - '.github/workflows/newman-comprehensive-tests.yml'
  pull_request:
    branches: [main, dev]
    paths:
      - 'tests/postman/collections/comprehensive.json'
      - 'tests/postman/collections/pagination-validation.json'
      - 'tests/postman/environments/comprehensive.json'
      - 'tests/postman/data/pagination-tests.json'
      - 'scripts/run-newman-comprehensive.sh'
      - 'scripts/run-newman-pagination-validation.sh'
      - 'scripts/analyze-newman-regression.js'
      - 'scripts/test-schema.sql'
      - 'scripts/test-seed-data.sql'
      - 'server/**'
      - 'routes/**'
      - 'lib/**'
      - '.github/workflows/newman-comprehensive-tests.yml'
  workflow_dispatch:
    inputs:
      test_folder:
        description: 'Specific test folder to run (e.g., "Stamps Endpoints")'
        required: false
        default: ''
      iterations:
        description: 'Number of iterations'
        required: false
        default: '1'
      verbose:
        description: 'Enable verbose output'
        required: false
        default: 'false'
  schedule:
    # Run daily at 2 AM UTC to validate production endpoints
    - cron: '0 2 * * *'

jobs:
  newman-comprehensive:
    name: Newman Integration Tests (Database Required)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    permissions:
      contents: read
      pull-requests: write
      issues: write
      
    env:
      CI: true
      NODE_ENV: test
      # Mock API keys for testing
      QUICKNODE_API_KEY: test-quicknode-key
      ANTHROPIC_API_KEY: test-anthropic-key
      PERPLEXITY_API_KEY: test-perplexity-key

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache npm dependencies
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-newman-${{ hashFiles('**/package.json') }}
          restore-keys: |
            ${{ runner.os }}-npm-newman-
            ${{ runner.os }}-npm-

      - name: Verify test files exist
        run: |
          echo "Verifying test collection and environment files..."
          if [ ! -f "tests/postman/collections/comprehensive.json" ]; then
            echo "‚ùå Error: tests/postman/collections/comprehensive.json not found"
            echo "This file contains the comprehensive test collection (100+ tests)"
            exit 1
          fi
          
          if [ ! -f "tests/postman/environments/comprehensive.json" ]; then
            echo "‚ùå Error: tests/postman/environments/comprehensive.json not found"
            echo "Creating default environment file..."
            mkdir -p tests/postman/environments
            echo '{
              "id": "comprehensive-env",
              "name": "Comprehensive Test Environment",
              "values": [
                {"key": "dev_base_url", "value": "http://localhost:8000", "enabled": true},
                {"key": "prod_base_url", "value": "https://stampchain.io", "enabled": true}
              ]
            }' > tests/postman/environments/comprehensive.json
          fi
          
          echo "‚úÖ Test files found"
          echo "Collection size: $(wc -l < tests/postman/collections/comprehensive.json) lines"

      - name: Setup Docker Compose
        run: |
          echo "Docker version:"
          docker --version
          echo "Docker Compose version:"
          docker compose version
          
          # Ensure reports directory exists
          mkdir -p reports/newman-comprehensive
          chmod -R 755 reports

      - name: Configure test parameters
        id: test-config
        run: |
          # Set test parameters based on workflow inputs or defaults
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "NEWMAN_FOLDER=${{ github.event.inputs.test_folder }}" >> $GITHUB_ENV
            echo "NEWMAN_ITERATIONS=${{ github.event.inputs.iterations }}" >> $GITHUB_ENV
            echo "NEWMAN_VERBOSE=${{ github.event.inputs.verbose }}" >> $GITHUB_ENV
          else
            echo "NEWMAN_ITERATIONS=1" >> $GITHUB_ENV
            echo "NEWMAN_VERBOSE=false" >> $GITHUB_ENV
          fi
          
          # Always set production URL
          echo "PROD_BASE_URL=https://stampchain.io" >> $GITHUB_ENV
          
          # CI Mode: Since we don't have a dev server running in CI,
          # we'll run production-only tests but still use the dual-endpoint collection
          # The collection will just hit production for both dev and prod URLs
          echo "DEV_BASE_URL=https://stampchain.io" >> $GITHUB_ENV
          echo "TEST_MODE=ci-production-only" >> $GITHUB_ENV
          
          # Note: For local development with dev server running, users should run:
          # DEV_BASE_URL=http://localhost:8000 PROD_BASE_URL=https://stampchain.io docker compose -f docker-compose.test.yml run --rm newman-comprehensive

      - name: Run Newman Comprehensive Tests
        id: newman-test
        run: |
          {
            echo "## Newman Integration Test Run"
            echo ""
            echo "‚ö†Ô∏è **Integration Test**: Tests live API endpoints with database access"
            echo ""
            echo "### Test Configuration"
            echo "- Collection: tests/postman/collections/comprehensive.json (100+ tests)"
            echo "- Coverage: 46/46 endpoints (100%)"
            echo "- Mode: ${{ env.TEST_MODE }}"
            echo "- DEV_BASE_URL: ${{ env.DEV_BASE_URL }}"
            echo "- PROD_BASE_URL: ${{ env.PROD_BASE_URL }}"
            echo "- Iterations: ${{ env.NEWMAN_ITERATIONS }}"
            
            if [ -n "${{ env.NEWMAN_FOLDER }}" ]; then
              echo "- Folder: ${{ env.NEWMAN_FOLDER }}"
            fi
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
          
          # Run the tests
          docker compose -f docker-compose.test.yml run --rm newman-comprehensive || TEST_EXIT_CODE=$?
          
          # Check for test results
          if [ -d "reports/newman-comprehensive" ]; then
            {
              echo "### Test Results"
              echo "Reports generated in: reports/newman-comprehensive/"
              ls -la reports/newman-comprehensive/
            } >> "$GITHUB_STEP_SUMMARY"
          fi
          
          exit ${TEST_EXIT_CODE:-0}
        continue-on-error: true

      - name: Fix Docker file permissions
        if: always()
        run: |
          # Docker containers run as root and change file ownership
          # This prevents EACCES errors in subsequent steps
          sudo chown -R $(id -u):$(id -g) . || true

      - name: Analyze Regression Results
        id: regression-analysis
        if: always()
        run: |
          echo "Running regression analysis..."

          # Run the analysis (uses only Node.js built-in modules, no npm install needed)
          node scripts/analyze-newman-regression.js || ANALYSIS_EXIT_CODE=$?

          # Check if analysis report was created
          if ls reports/newman-comprehensive/*-analysis.json 1>/dev/null 2>&1; then
            echo "‚úÖ Regression analysis completed"

            # Extract summary for GitHub
            LATEST_ANALYSIS=$(ls -t reports/newman-comprehensive/*-analysis.json | head -1)
            if [ -f "$LATEST_ANALYSIS" ]; then
              {
                echo ""
                echo "### Regression Analysis Summary"
                echo ""

                # Parse JSON and add to summary
                node -e "
                  const fs = require('fs');
                  const analysis = JSON.parse(fs.readFileSync('$LATEST_ANALYSIS', 'utf8'));
                  console.log('- Breaking Changes: ' + analysis.summary.breaking);
                  console.log('- Non-Breaking Changes: ' + analysis.summary.nonBreaking);
                  console.log('- Performance Issues: ' + analysis.summary.performanceIssues);
                  console.log('- Performance Improvements: ' + analysis.summary.performanceImprovements);
                "
              } >> "$GITHUB_STEP_SUMMARY"
            fi
          else
            echo "‚ö†Ô∏è No analysis reports found ‚Äî Newman may not have generated output files"
          fi

          exit ${ANALYSIS_EXIT_CODE:-0}
        continue-on-error: true

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: newman-comprehensive-reports-${{ github.run_number }}
          path: reports/newman-comprehensive/
          retention-days: 30

      - name: Create PR comment with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            let comment = '## üß™ Newman Integration Test Results (API)\n\n';
            comment += '‚ö†Ô∏è **Integration Test**: These tests hit live API endpoints and require database access.\n\n';
            
            // Test execution status
            const testStatus = '${{ steps.newman-test.outcome }}';
            const analysisStatus = '${{ steps.regression-analysis.outcome }}';
            
            if (testStatus === 'success') {
              comment += '### ‚úÖ All API Tests Passed\n\n';
              comment += '- **Coverage**: 46/46 endpoints (100%)\n';
              comment += '- **Total Tests**: 92+\n';
            } else if (testStatus === 'failure') {
              comment += '### ‚ùå API Tests Failed\n\n';
              comment += 'Some tests did not pass. Please check the detailed report.\n\n';
            }
            
            // Regression analysis results
            if (analysisStatus === 'success' || analysisStatus === 'failure') {
              comment += '\n### üìä Regression Analysis\n\n';
              
              try {
                // Find the latest analysis file
                const analysisFiles = fs.readdirSync('reports/newman-comprehensive/')
                  .filter(f => f.endsWith('-analysis.json'))
                  .sort((a, b) => b.localeCompare(a));
                
                if (analysisFiles.length > 0) {
                  const latestAnalysis = JSON.parse(
                    fs.readFileSync(`reports/newman-comprehensive/${analysisFiles[0]}`, 'utf8')
                  );
                  
                  const summary = latestAnalysis.summary;
                  
                  if (summary.breaking > 0) {
                    comment += `‚ö†Ô∏è **Breaking Changes Detected**: ${summary.breaking}\n\n`;
                    
                    // List breaking changes
                    if (latestAnalysis.regressions.breaking.length > 0) {
                      comment += '<details><summary>Breaking Changes Details</summary>\n\n';
                      latestAnalysis.regressions.breaking.slice(0, 5).forEach(bc => {
                        comment += `- **${bc.request}**: ${bc.assertion}\n`;
                        comment += `  - Error: ${bc.error}\n`;
                      });
                      if (latestAnalysis.regressions.breaking.length > 5) {
                        comment += `\n... and ${latestAnalysis.regressions.breaking.length - 5} more\n`;
                      }
                      comment += '\n</details>\n\n';
                    }
                  } else {
                    comment += '‚úÖ **No Breaking Changes**\n\n';
                  }
                  
                  // Non-breaking changes
                  if (summary.nonBreaking > 0) {
                    comment += `‚ÑπÔ∏è **Non-Breaking Changes**: ${summary.nonBreaking}\n`;
                    comment += 'These are typically new optional fields or formatting changes.\n\n';
                  }
                  
                  // Performance
                  if (summary.performanceIssues > 0) {
                    comment += `‚ö° **Performance Issues**: ${summary.performanceIssues}\n`;
                  }
                  if (summary.performanceImprovements > 0) {
                    comment += `üöÄ **Performance Improvements**: ${summary.performanceImprovements}\n`;
                  }
                }
              } catch (e) {
                comment += 'Could not parse regression analysis results.\n';
                console.error('Error parsing analysis:', e);
              }
            }
            
            // Add links
            comment += '\n### üìé Resources\n\n';
            comment += '- [Test Reports Artifact](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
            comment += '- [Newman Testing Documentation](docs/NEWMAN_COMPREHENSIVE_TESTING.md)\n';
            comment += '- [API Endpoint Coverage](docs/API_ENDPOINT_AUDIT.md)\n';
            
            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('Newman Integration Test Results')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Summarize results
        if: always()
        run: |
          echo "=== Newman Comprehensive Test Summary ==="

          # Report Newman test status
          if [ "${{ steps.newman-test.outcome }}" == "failure" ]; then
            echo "‚ö†Ô∏è Newman tests had assertion failures (check step summary for details)"
            echo "   Note: /api/internal/* endpoints return 403 by design in CI"
          else
            echo "‚úÖ Newman tests passed"
          fi

          # Report regression analysis status
          if [ "${{ steps.regression-analysis.outcome }}" == "failure" ]; then
            echo "‚ö†Ô∏è Regression analysis flagged issues (check artifacts for report)"
          else
            echo "‚úÖ Regression analysis completed"
          fi

          # This workflow is informational (scheduled monitoring only)
          # Results are in step summary and uploaded artifacts
          echo "‚úÖ Monitoring run complete"

  pagination-validation:
    name: Pagination & Data Validation Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || github.event_name == 'push'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Newman
        run: |
          npm install -g newman newman-reporter-html newman-reporter-json

      - name: Run Pagination Validation Tests
        id: pagination-test
        run: |
          echo "## Pagination Validation Test Run" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- Collection: tests/postman/collections/pagination-validation.json" >> $GITHUB_STEP_SUMMARY
          echo "- Test Data: postman-data-pagination-tests.json" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Run pagination validation tests
          ./scripts/run-newman-pagination-validation.sh || PAGINATION_EXIT_CODE=$?
          
          exit ${PAGINATION_EXIT_CODE:-0}
        continue-on-error: true

      - name: Upload pagination test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pagination-validation-reports-${{ github.run_number }}
          path: reports/newman-pagination-validation/
          retention-days: 30

  performance-benchmark:
    name: API Performance Benchmarking
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run performance benchmarks
        id: perf-test
        run: |
          echo "## Performance Benchmarking" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- Iterations: 3" >> $GITHUB_STEP_SUMMARY
          echo "- Delay between requests: 100ms" >> $GITHUB_STEP_SUMMARY
          echo "- Focus: Stamps Endpoints folder" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Run performance-focused tests against production
          # (no dev server in CI, point both URLs at production)
          DEV_BASE_URL=https://stampchain.io \
          PROD_BASE_URL=https://stampchain.io \
          NEWMAN_ITERATIONS=3 \
          NEWMAN_DELAY_REQUEST=100 \
          NEWMAN_FOLDER="Stamps Endpoints" \
          docker compose -f docker-compose.test.yml run --rm newman-comprehensive
        continue-on-error: true

      - name: Fix Docker file permissions
        if: always()
        run: sudo chown -R $(id -u):$(id -g) . || true

      - name: Store performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmark-${{ github.run_number }}
          path: reports/newman-comprehensive/
          retention-days: 90

      - name: Summarize benchmark results
        if: always()
        run: |
          echo "=== Performance Benchmark Summary ==="
          if [ "${{ steps.perf-test.outcome }}" == "failure" ]; then
            echo "‚ö†Ô∏è Some benchmark assertions failed (check artifacts for details)"
            echo "   Note: /api/internal/* endpoints return 403 by design in CI"
          else
            echo "‚úÖ Performance benchmarks passed"
          fi
          echo "‚úÖ Benchmark run complete"

  newman-local-dev:
    name: Newman Tests - Local Dev Server
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'workflow_dispatch'

    permissions:
      contents: read
      pull-requests: write
      issues: write

    services:
      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: test
          MYSQL_DATABASE: btcstamps_test
        ports:
          - 3306:3306
        options: >-
          --health-cmd="mysqladmin ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    env:
      CI: true
      DENO_ENV: development
      # Database configuration
      DB_HOST: 127.0.0.1
      DB_PORT: 3306
      DB_USER: root
      DB_PASSWORD: test
      DB_NAME: btcstamps_test
      # Redis configuration
      REDIS_URL: redis://127.0.0.1:6379
      ELASTICACHE_ENDPOINT: 127.0.0.1:6379
      SKIP_REDIS_CONNECTION: false
      # Mock API keys for testing
      QUICKNODE_API_KEY: test-quicknode-key
      ANTHROPIC_API_KEY: test-anthropic-key
      PERPLEXITY_API_KEY: test-perplexity-key
      # Mock external APIs for POST endpoint testing (Counterparty, mempool.space, Blockstream)
      XCP_API_URL: http://localhost:18443/v2
      MEMPOOL_API_URL: http://localhost:18443/mempool/api
      BLOCKSTREAM_API_URL: http://localhost:18443/blockstream/api

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.6.9

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache Deno dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.deno
            ~/.cache/deno
          key: ${{ runner.os }}-deno-newman-${{ hashFiles('**/deno.json', '**/import_map.json') }}
          restore-keys: |
            ${{ runner.os }}-deno-newman-
            ${{ runner.os }}-deno-

      - name: Cache npm dependencies
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-newman-${{ hashFiles('**/package.json') }}
          restore-keys: |
            ${{ runner.os }}-npm-newman-
            ${{ runner.os }}-npm-

      - name: Wait for MySQL
        run: |
          echo "Waiting for MySQL to be ready..."
          timeout 60 bash -c 'until mysqladmin ping -h 127.0.0.1 -P 3306 -u root -ptest --silent; do
            echo "MySQL not ready yet, waiting..."
            sleep 2
          done'
          echo "‚úÖ MySQL is ready"

      - name: Wait for Redis
        run: |
          echo "Waiting for Redis to be ready..."
          timeout 30 bash -c 'until nc -z 127.0.0.1 6379; do
            echo "Redis not ready yet, waiting..."
            sleep 1
          done'
          echo "‚úÖ Redis is ready"

      - name: Load test schema
        run: |
          echo "Loading test schema..."
          mysql -h 127.0.0.1 -P 3306 -u root -ptest btcstamps_test < scripts/test-schema.sql
          echo "‚úÖ Test schema loaded"

      - name: Load test seed data
        run: |
          echo "Loading test seed data..."
          mysql -h 127.0.0.1 -P 3306 -u root -ptest btcstamps_test < scripts/test-seed-data.sql
          echo "‚úÖ Test seed data loaded"

      - name: Verify test data
        run: |
          echo "Verifying test data..."
          mysql -h 127.0.0.1 -P 3306 -u root -ptest btcstamps_test -e "SHOW TABLES;"
          mysql -h 127.0.0.1 -P 3306 -u root -ptest btcstamps_test -e "SELECT COUNT(*) AS stamp_count FROM StampTableV4;"
          mysql -h 127.0.0.1 -P 3306 -u root -ptest btcstamps_test -e "SELECT COUNT(*) AS block_count FROM blocks;"
          echo "‚úÖ Test data verified"

      - name: Cache Deno dependencies for dev server
        run: |
          echo "Caching Deno dependencies..."
          deno cache --no-check main.ts dev.ts

      - name: Start mock external API server
        run: |
          echo "Starting mock external API server for POST endpoint testing..."
          nohup deno run --allow-net --allow-env scripts/mock-external-apis.ts > mock-api-server.log 2>&1 &
          echo $! > mock-api-server.pid
          echo "Mock server started with PID: $(cat mock-api-server.pid)"

          # Wait for mock server to be ready
          timeout 15 bash -c 'until curl -f -s http://localhost:18443/health > /dev/null 2>&1; do
            echo "Mock server not ready yet, waiting..."
            sleep 1
          done' || {
            echo "‚ùå Mock API server failed to start"
            cat mock-api-server.log || echo "No logs available"
            exit 1
          }
          echo "‚úÖ Mock external API server is ready"

      - name: Start dev server
        run: |
          echo "Starting Deno dev server in background..."
          # Start dev server without watch mode for CI
          # XCP_API_URL, MEMPOOL_API_URL, BLOCKSTREAM_API_URL from job env
          # point to mock server for POST endpoint testing
          DENO_ENV=development \
          DB_HOST=127.0.0.1 \
          DB_USER=root \
          DB_PASSWORD=test \
          DB_NAME=btcstamps_test \
          REDIS_URL=redis://127.0.0.1:6379 \
          XCP_API_URL=http://localhost:18443/v2 \
          MEMPOOL_API_URL=http://localhost:18443/mempool/api \
          BLOCKSTREAM_API_URL=http://localhost:18443/blockstream/api \
          nohup deno run -A dev.ts > dev-server.log 2>&1 &

          echo "Dev server started with PID: $!"
          echo $! > dev-server.pid

          echo "Waiting for dev server to be ready (max 60s)..."
          timeout 60 bash -c 'until curl -f -s http://localhost:8000/api/v2/health > /dev/null 2>&1; do
            echo "Dev server not ready yet, waiting..."
            sleep 2
          done' || {
            echo "‚ùå Dev server failed to start within 60 seconds"
            echo "=== Dev Server Logs ==="
            cat dev-server.log || echo "No logs available"
            exit 1
          }

          echo "‚úÖ Dev server is ready at http://localhost:8000"

      - name: Verify dev server health
        run: |
          echo "Checking dev server health endpoint..."
          curl -f http://localhost:8000/api/v2/health || {
            echo "‚ùå Health check failed"
            exit 1
          }
          echo "‚úÖ Health check passed"

      - name: Install Newman
        run: |
          echo "Installing Newman and reporters..."
          npm install -g newman newman-reporter-html newman-reporter-json

      - name: Verify test collection exists
        run: |
          echo "Verifying test collection and environment files..."
          if [ ! -f "tests/postman/collections/comprehensive.json" ]; then
            echo "‚ùå Error: tests/postman/collections/comprehensive.json not found"
            exit 1
          fi

          if [ ! -f "tests/postman/environments/comprehensive.json" ]; then
            echo "‚ö†Ô∏è Warning: tests/postman/environments/comprehensive.json not found"
            echo "Tests will run with dev_base_url as environment variable"
          fi

          echo "‚úÖ Test files verified"

      - name: Run Newman tests against localhost
        id: newman-local
        run: |
          echo "Running Newman tests against local dev server..."
          mkdir -p reports/newman-local-dev

          # Run Newman tests pointing to localhost with comprehensive seed data
          newman run tests/postman/collections/comprehensive.json \
            --env-var dev_base_url=http://localhost:8000 \
            --env-var prod_base_url=http://localhost:8000 \
            --env-var baseUrl=http://localhost:8000 \
            --env-var test_block=820000 \
            --env-var test_stamp_id=1384305 \
            --env-var test_cpid=A888354448084788958 \
            --env-var test_address=bc1qkqqre5xuqk60xtt93j297zgg7t6x0ul7gwjmv4 \
            --env-var test_tx_hash=e94be2793462692ca8fea3a54dd90ff4b18735196a2bc426382c11959533c8ca \
            --env-var test_src20_tick=stamp \
            --env-var test_cursed_id=-1832 \
            --env-var test_deploy_hash=77fb147b72a551cf1e2f0b37dccf9982a1c25623a7fe8b4d5efaac566cf63fed \
            --env-var test_tokenid=U0FUT1NISU5BS0FNT1RP \
            --env-var test_index=1942 \
            --env-var test_tick=stamp \
            --env-var test_timeout=10000 \
            --reporters cli,html,json \
            --reporter-html-export reports/newman-local-dev/newman-report.html \
            --reporter-json-export reports/newman-local-dev/newman-report.json \
            || NEWMAN_EXIT_CODE=$?

          echo "Newman tests completed"

          # Create summary
          {
            echo "## Newman Local Dev Test Results"
            echo ""
            echo "### Configuration"
            echo "- Base URL: http://localhost:8000"
            echo "- Collection: tests/postman/collections/comprehensive.json"
            echo "- Environment: Local dev server with MySQL + Redis"
            echo ""

            if [ -f "reports/newman-local-dev/newman-report.json" ]; then
              echo "### Results Summary"
              node -e "
                const fs = require('fs');
                const report = JSON.parse(fs.readFileSync('reports/newman-local-dev/newman-report.json', 'utf8'));
                const stats = report.run.stats;
                console.log('- Total Requests: ' + stats.requests.total);
                console.log('- Passed Assertions: ' + stats.assertions.passed);
                console.log('- Failed Assertions: ' + stats.assertions.failed);
                console.log('- Total Assertions: ' + stats.assertions.total);
                console.log('- Average Response Time: ' + Math.round(report.run.timings.responseAverage) + 'ms');
              "
            fi
          } >> "$GITHUB_STEP_SUMMARY"

          exit ${NEWMAN_EXIT_CODE:-0}
        continue-on-error: false

      - name: Capture dev server logs on failure
        if: failure()
        run: |
          echo "=== Dev Server Logs ==="
          cat dev-server.log || echo "No logs available"

      - name: Stop dev server and mock API server
        if: always()
        run: |
          if [ -f dev-server.pid ]; then
            echo "Stopping dev server..."
            kill $(cat dev-server.pid) || true
            rm dev-server.pid
          fi
          if [ -f mock-api-server.pid ]; then
            echo "Stopping mock API server..."
            kill $(cat mock-api-server.pid) || true
            rm mock-api-server.pid
          fi

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: newman-local-dev-reports-${{ github.run_number }}
          path: reports/newman-local-dev/
          retention-days: 30

      - name: Upload dev server logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: dev-server-logs-${{ github.run_number }}
          path: dev-server.log
          retention-days: 7

      - name: Create PR comment with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            let comment = '## üß™ Newman Local Dev Test Results\n\n';
            comment += '‚úÖ **Local Dev Server**: Tests run against localhost:8000 with MySQL + Redis\n\n';

            const testStatus = '${{ steps.newman-local.outcome }}';

            if (testStatus === 'success') {
              comment += '### ‚úÖ All Local Dev Tests Passed\n\n';

              try {
                const report = JSON.parse(fs.readFileSync('reports/newman-local-dev/newman-report.json', 'utf8'));
                const stats = report.run.stats;

                comment += `- **Total Requests**: ${stats.requests.total}\n`;
                comment += `- **Passed Assertions**: ${stats.assertions.passed}\n`;
                comment += `- **Failed Assertions**: ${stats.assertions.failed}\n`;
                comment += `- **Total Assertions**: ${stats.assertions.total}\n`;
                comment += `- **Average Response Time**: ${Math.round(report.run.timings.responseAverage)}ms\n`;
              } catch (e) {
                comment += 'Could not parse test results.\n';
              }
            } else {
              comment += '### ‚ùå Local Dev Tests Failed\n\n';
              comment += 'Some tests did not pass against the local dev server. Please check the detailed report.\n\n';
            }

            comment += '\n### üìé Resources\n\n';
            comment += '- [Test Reports Artifact](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Newman Local Dev Test Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Summarize results
        if: always()
        run: |
          echo "=== Newman Local Dev Test Summary ==="

          if [ "${{ steps.newman-local.outcome }}" == "success" ]; then
            echo "‚úÖ Newman tests passed against local dev server"
          else
            echo "‚ùå Newman tests failed against local dev server"
          fi

          echo "‚úÖ Local dev test run complete"